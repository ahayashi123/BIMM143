---
title: "Class 7: Machine Learning 1"
author: "Alyssa Hayashi"
format: pdf
---

# Clustering Methods

The broad goal here is to find groupings (clusters) in your input data.

##Kmeans 

First, let's make up somee data to cluster.

```{r}
x<- rnorm(1000)
hist(x)
```

Make a vector of length 60 with 30 points centered at -3 and 30 points centered at +3
```{r}
temp<-c(rnorm(30, mean=-3), rnorm(30, mean=3))
temp

```

I will now make a wee x and y dataset with 2 groups of points 

```{r}
rev(c(1:5))
```

```{r}
x<- cbind(x=temp, y=rev(temp))
plot(x)

```

```{r}
k<- kmeans(x, center=2)
k
```

> Q. from your result object 'k' how many points are in each cluster?

```{r}
k$size
```

> Q. What "component" of your result object details the cluter membership.

```{r}
k$cluster
```

> Cluster centers

```{r}
k$centers
```

> Plot of our clustering results 

```{r}
plot(x, col=k$cluster)
points(k$centers, col="blue", pch=15, cex=2)
```

We can cluster into 4 groups 
```{r}
#kmeans
k4 <- kmeans(x, center=4)
#plot results 
plot(x, col=k4$cluster)
```

limitation of kmeans is that it does what you ask even if you ask silly clusters

## Hierarchical Clustering

The maain base R function for Hierarchical Clustering is 'hclust'. Unlike 'kmeans()' you can not just pass it your data as input.. You first need to calculate a distance matrix.


```{r}
d<- dist(x)
hc<- hclust(d)
```

use `plot()` to view results 

```{r}
plot(hc)
abline(h=10, col= "red")
```
Make the cut and get our clusster membership vector we can use the cutree() function

```{r}
grps<- cutree(hc, h=10)
grps
```

Make a plot of our data colored by hclust results

```{r}
plot( x, col=grps)
```

# Principle Component Analysis 

Here we will do Principal Component Analysis (PCA) on food data from the uk 

>Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

USing the row.names=1 approach is preferred since the second one results in a row being deleted and if ran multiple times rows that are desired will get deleted. 

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1)
```

```{r}
#rownaames(x) x[,1]
#x<- x[,-1]
#x
```

> Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
nrow(x)
ncol(x)
```

>Q3: Changing what optional argument in the above barplot() function results in the following plot?

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))

#changing the beside argument from T to F causes the bar plot to a stacked bar plot. 
```

>Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col=rainbow(10), pch=16)

```

## PCA to the rescue 

The main base R function for PCA is called `prcomp()`

```{r}
pca<- prcomp(t(x))
summary(pca)
```

> Q. How much varriance is captured in 2 pcs

 96.5%
 
 
 To make or main "PC score plot" or "PC1 vs PC2 plot" or "PC plot" or "ordinated plot"
 
```{r}
attributes(pca)
```
 
 We are after the `pca$x` result component to make. or man PCA plot 
 
```{r}
pca$x
```
 
```{r}
mycol<- c("orange", "red", "blue", "darkgreen")
plot(pca$x[,1], pca$x[,2], col=mycol, pch=16, xlab= "PC1 (67.4%)", ylab= "PC (29%)")
```
 
 Another important result from pca is how the original variables (in this case the food) contributes to the pca 
 
 This is contained in the `pca$rotation` object - folks often call this the "loadings" or "contributions" to the PCs
 
```{r}
pca$rotation
```
 
 We can make a plot along PC1
 
```{r}
library(ggplot2)
contributions<- as.data.frame(pca$rotation)
ggplot(contributions)
```
 
```{r}
ld <- as.data.frame(pca$rotation)
ld_lab <- tibble::rownames_to_column(ld, "Food")

ggplot(ld_lab) +
  aes(PC1, Food) +
  geom_col() 
```
 

 